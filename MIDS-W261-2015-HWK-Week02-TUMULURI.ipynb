{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gopala Tumuluri's Submission for HW2 and Parts of HW1\n",
    "\n",
    "*NOTE: I replicated a lot of 'reducer' code across problems to make it easy for the grader. I would never write redundant code in this fashion. Please be considerate on this point.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.0/libexec/logs/yarn-gtumuluri-resourcemanager-GTA2.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.0/libexec/logs/yarn-gtumuluri-nodemanager-GTA2.out\n",
      "15/09/15 17:46:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.0/libexec/logs/hadoop-gtumuluri-namenode-GTA2.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.0/libexec/logs/hadoop-gtumuluri-datanode-GTA2.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.0/libexec/logs/hadoop-gtumuluri-secondarynamenode-GTA2.out\n",
      "15/09/15 17:46:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.0/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.0/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a folder in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 17:46:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/gtumuluri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem - 1.0.0\n",
    "\n",
    "#### Define big data. Provide an example of a big data problem in your domain of expertise.\n",
    "\n",
    "In my mind, as a technology person, big data means content (loosely to mean bits/bytes) of value that don't fit on a single or a small set of machines, and certainly can't be computed on such resources. Data that requires multiple computers to store and compute qualifies as big data in my view.\n",
    "\n",
    "I work in the healthcare industry where we bill and collect payments from patients online on behalf of hospitals and physicians offices. In my line work, I am routinely task with processing multiple gigabytes worth of web log data. This has not risen to the level of big data yet since we are a growing startup. But, with the exponential user base growth, I expect this problem to become a nice big data problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem - 1.0.1\n",
    "\n",
    "#### Bias and Variance\n",
    "\n",
    "First let me address the irreducible error. This is the error that is inherent in any randomly produced/occurring/generated data. The variation that is purely due to chance and can't be eliminated/reduced by any model, especially for unseen observations. For any meaningful amount of data, a perfect fit can not be achieved even when using extremely high order polynomials.\n",
    "\n",
    "Bias occurs when the model is under fit due to poor or limited features. This will manifest itself in the form of high training and test erorr. If one were to plot the curves in a reflected manner, they would see that the train and test errors are far apart from zero, and apart from each other. To address bias, one has to go for a more complex model.\n",
    "\n",
    "Variance problem occurs when the model is overfit on the training data and perhaps performs perfectly on this data. But, has very large error when predicting test / unseen data. Using high order polynomials can result in training data being so 'well' fit that the error is minimal (near zero), but when the same model is used to predict test/new data, the error rate is very high. When you plot the erorr rates for train and test data, a high variance model will show that the train data has near zero error, and the test data will have a very high error rate, and even a growing error rate as the overfitting continues.\n",
    "\n",
    "Model selection - First, plotting error rates and observing trends is crucial. One must select a model that strikes a good balance between train/test error. If the train error can't be reduced by much, one must consider this to be a bias problem and focus on additional feature selection. If the train error is low, but the test error starts to diverge away from zero, this should be considered a high variance problem, and the model should be simplified to not overfit the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem - 1.1: Read through pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.0\n",
    "\n",
    "#### Race Condition\n",
    "\n",
    "A race condition in programming context is when one process that depends on a step, essentially races past it before that milestone has been met by yet another co-dependent process. This would result in unpredictable outputs. A simple example would be the map-reduce one where some mappers finish and if the reducer were to start its work before all mappers finished, the results would be unpredictable and wrong.\n",
    "\n",
    "#### MapReduce\n",
    "\n",
    "MapReduce is a parallel, functional programming framework that allows for share-nothing distributed processing (to solve embarrassingly parallel problems). It differs from Hadoop greatly. Hadoop is a platform for stroing data in a highly distributed fashion with replication, redundancy and fault tolerance, and also allowing programs to operate on that data through centralized management and control. MapReduce on the other hand is an abstraction on top of Hadoop to actally perform the necessary computation on the data.\n",
    "\n",
    "#### MapReduce Programming Paradigm\n",
    "\n",
    "MapReduce uses functional programming paradigm where one can write functions for map and reduce, and have those functions be applied to data in a specific step/order. In anotherway, mapreduce is also a 'gateway' type programming model where there are wait stages to ensure all parallel tasks complete a step before proceeding to the next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1 - Random Number Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate 10,000 random integers between 0 and some large number\n",
    "nums = [random.randint(0, 1000000) for i in range(0, 10000)]\n",
    "\n",
    "# write them one line at a time to a file as key value\n",
    "# in the format of 'number, NA'\n",
    "file = open('2.1_randints.txt', 'w')\n",
    "for num in nums:\n",
    "    file.writelines(str(num) + ', NA\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1 - Hadoop Mapper for Sorting Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2.1_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.1_mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# Simply read the input from standard inpit and output\n",
    "# the number and the count (1) - the latter does not matter.\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split(',')\n",
    "    print '%s\\t%s' % (words[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1 - Hadoop Reducer for Sorting Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2.1_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.1_reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# Simply read the standard input and output the value\n",
    "# It comes in sorted order from the hadoop shuffle step\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    print words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 2.1 - File Upload of 10,000 Random Integers to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 17:47:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put 2.1_randints.txt /user/gtumuluri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1 - Run Hadoop MapReduce with Key Comparator Option to Perform Numeric Sort\n",
    "\n",
    "Hadoop framework sorts key/value pairs output from the mappers using alphabetical sort order. This won't work for sorting integers. So, we change the key comparator to use a numeric sort in the shuffle phase so that the numbers appear in numerically sorted order at the reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 17:48:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 17:48:01 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 17:48:01 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 17:48:01 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 17:48:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 17:48:02 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 17:48:02 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "15/09/15 17:48:02 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "15/09/15 17:48:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local540527564_0001\n",
      "15/09/15 17:48:02 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 17:48:02 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 17:48:02 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 17:48:02 INFO mapreduce.Job: Running job: job_local540527564_0001\n",
      "15/09/15 17:48:02 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 17:48:02 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 17:48:02 INFO mapred.LocalJobRunner: Starting task: attempt_local540527564_0001_m_000000_0\n",
      "15/09/15 17:48:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 17:48:03 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/15 17:48:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/15 17:48:03 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/gtumuluri/randints.txt:0+1090\n",
      "15/09/15 17:48:03 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 17:48:03 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 17:48:03 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 17:48:03 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 17:48:03 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 17:48:03 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 17:48:03 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/gtumuluri/Documents/BerkeleyMIDS/ScalableML/HW2/./2.1_mapper.py]\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 17:48:03 INFO mapred.LocalJobRunner: \n",
      "15/09/15 17:48:03 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 17:48:03 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 17:48:03 INFO mapred.MapTask: bufstart = 0; bufend = 890; bufvoid = 104857600\n",
      "15/09/15 17:48:03 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "15/09/15 17:48:03 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 17:48:03 INFO mapred.Task: Task:attempt_local540527564_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 17:48:03 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "15/09/15 17:48:03 INFO mapred.Task: Task 'attempt_local540527564_0001_m_000000_0' done.\n",
      "15/09/15 17:48:03 INFO mapred.LocalJobRunner: Finishing task: attempt_local540527564_0001_m_000000_0\n",
      "15/09/15 17:48:03 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 17:48:03 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 17:48:03 INFO mapred.LocalJobRunner: Starting task: attempt_local540527564_0001_r_000000_0\n",
      "15/09/15 17:48:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 17:48:03 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/15 17:48:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/15 17:48:03 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@70010924\n",
      "15/09/15 17:48:03 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 17:48:03 INFO reduce.EventFetcher: attempt_local540527564_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 17:48:03 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local540527564_0001_m_000000_0 decomp: 1092 len: 1096 to MEMORY\n",
      "15/09/15 17:48:03 INFO reduce.InMemoryMapOutput: Read 1092 bytes from map-output for attempt_local540527564_0001_m_000000_0\n",
      "15/09/15 17:48:03 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1092, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1092\n",
      "15/09/15 17:48:03 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 17:48:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 17:48:03 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 17:48:03 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 17:48:03 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1085 bytes\n",
      "15/09/15 17:48:03 INFO reduce.MergeManagerImpl: Merged 1 segments, 1092 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 17:48:03 INFO reduce.MergeManagerImpl: Merging 1 files, 1096 bytes from disk\n",
      "15/09/15 17:48:03 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 17:48:03 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 17:48:03 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1085 bytes\n",
      "15/09/15 17:48:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/gtumuluri/Documents/BerkeleyMIDS/ScalableML/HW2/./2.1_reducer.py]\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 17:48:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "15/09/15 17:48:03 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 17:48:03 INFO mapreduce.Job: Job job_local540527564_0001 running in uber mode : false\n",
      "15/09/15 17:48:03 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 17:48:03 INFO mapred.Task: Task:attempt_local540527564_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 17:48:03 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 17:48:03 INFO mapred.Task: Task attempt_local540527564_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 17:48:03 INFO output.FileOutputCommitter: Saved output of task 'attempt_local540527564_0001_r_000000_0' to hdfs://localhost:9000/user/gtumuluri/randintOutput/_temporary/0/task_local540527564_0001_r_000000\n",
      "15/09/15 17:48:03 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "15/09/15 17:48:03 INFO mapred.Task: Task 'attempt_local540527564_0001_r_000000_0' done.\n",
      "15/09/15 17:48:03 INFO mapred.LocalJobRunner: Finishing task: attempt_local540527564_0001_r_000000_0\n",
      "15/09/15 17:48:03 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 17:48:04 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 17:48:04 INFO mapreduce.Job: Job job_local540527564_0001 completed successfully\n",
      "15/09/15 17:48:04 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=214286\n",
      "\t\tFILE: Number of bytes written=801750\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2180\n",
      "\t\tHDFS: Number of bytes written=790\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=890\n",
      "\t\tMap output materialized bytes=1096\n",
      "\t\tInput split bytes=101\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=1096\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=6\n",
      "\t\tTotal committed heap usage (bytes)=491782144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1090\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=790\n",
      "15/09/15 17:48:04 INFO streaming.StreamJob: Output directory: randintOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D  mapred.text.key.comparator.options=-n -mapper 2.1_mapper.py -reducer 2.1_reducer.py -input randints.txt -output randintOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1 - Output of Sorted Numbers (Show First Few Lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 17:48:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "6177\t\n",
      "11787\t\n",
      "12822\t\n",
      "15793\t\n",
      "18537\t\n",
      "33585\t\n",
      "42023\t\n",
      "46559\t\n",
      "96013\t\n",
      "100725\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat randintOutput/part-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2 - Mapper to Count Single Word Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 2.2_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.2_mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "# Take input from the standard input\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    items = line.split('\\t')\n",
    "    # Look for the 'body' portion of the email message\n",
    "    if len(items) == 4:\n",
    "        words = items[3].split()\n",
    "        for word in words:\n",
    "            # If the word in the body matches user specified,\n",
    "            # output its occurrence\n",
    "            if word.find(sys.argv[1]) == 0:\n",
    "                print '%s\\t%s' % (word, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2 - Reducer to Count Single Word Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 2.2_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.2_reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# Initialize count, read lines from standard input\n",
    "# and add up all words seen, print word and count\n",
    "count = 0\n",
    "for line in sys.stdin:\n",
    "    if count == 0:\n",
    "        words = line.strip()\n",
    "        words = line.split()\n",
    "    count += 1\n",
    "print '%s\\t%s' % (words[0], count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 2.2 - File Upload of Email Data Set to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 17:59:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `/user/gtumuluri/enronemail_1h.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put enronemail_1h.txt /user/gtumuluri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2 - Run Hadoop MapReduce with User Input Word\n",
    "\n",
    "Passing 'assistance' as a user supplied word to the mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 17:59:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 17:59:35 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 17:59:35 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 17:59:35 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 17:59:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 17:59:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 17:59:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local860394097_0001\n",
      "15/09/15 17:59:36 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 17:59:36 INFO mapreduce.Job: Running job: job_local860394097_0001\n",
      "15/09/15 17:59:36 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 17:59:36 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 17:59:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 17:59:36 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 17:59:36 INFO mapred.LocalJobRunner: Starting task: attempt_local860394097_0001_m_000000_0\n",
      "15/09/15 17:59:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 17:59:36 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/15 17:59:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/15 17:59:36 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/gtumuluri/enronemail_1h.txt:0+203981\n",
      "15/09/15 17:59:36 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 17:59:36 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 17:59:36 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 17:59:36 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 17:59:36 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 17:59:36 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 17:59:36 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 17:59:36 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/gtumuluri/Documents/BerkeleyMIDS/ScalableML/HW2/./2.2_mapper.py, assistance]\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 17:59:36 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 17:59:36 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 17:59:36 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 17:59:36 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 17:59:36 INFO streaming.PipeMapRed: Records R/W=101/1\n",
      "15/09/15 17:59:36 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 17:59:36 INFO mapred.LocalJobRunner: \n",
      "15/09/15 17:59:36 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 17:59:36 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 17:59:36 INFO mapred.MapTask: bufstart = 0; bufend = 107; bufvoid = 104857600\n",
      "15/09/15 17:59:36 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/6553600\n",
      "15/09/15 17:59:36 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 17:59:36 INFO mapred.Task: Task:attempt_local860394097_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 17:59:36 INFO mapred.LocalJobRunner: Records R/W=101/1\n",
      "15/09/15 17:59:36 INFO mapred.Task: Task 'attempt_local860394097_0001_m_000000_0' done.\n",
      "15/09/15 17:59:36 INFO mapred.LocalJobRunner: Finishing task: attempt_local860394097_0001_m_000000_0\n",
      "15/09/15 17:59:36 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 17:59:36 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 17:59:36 INFO mapred.LocalJobRunner: Starting task: attempt_local860394097_0001_r_000000_0\n",
      "15/09/15 17:59:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 17:59:36 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/15 17:59:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/15 17:59:36 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1187ae78\n",
      "15/09/15 17:59:36 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 17:59:36 INFO reduce.EventFetcher: attempt_local860394097_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 17:59:36 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local860394097_0001_m_000000_0 decomp: 125 len: 129 to MEMORY\n",
      "15/09/15 17:59:36 INFO reduce.InMemoryMapOutput: Read 125 bytes from map-output for attempt_local860394097_0001_m_000000_0\n",
      "15/09/15 17:59:36 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 125, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->125\n",
      "15/09/15 17:59:36 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 17:59:36 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 17:59:36 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 17:59:36 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 17:59:36 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 112 bytes\n",
      "15/09/15 17:59:36 INFO reduce.MergeManagerImpl: Merged 1 segments, 125 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 17:59:36 INFO reduce.MergeManagerImpl: Merging 1 files, 129 bytes from disk\n",
      "15/09/15 17:59:36 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 17:59:36 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 17:59:36 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 112 bytes\n",
      "15/09/15 17:59:36 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 17:59:36 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/gtumuluri/Documents/BerkeleyMIDS/ScalableML/HW2/./2.2_reducer.py]\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 17:59:36 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 17:59:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 17:59:37 INFO streaming.PipeMapRed: Records R/W=8/1\n",
      "15/09/15 17:59:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 17:59:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 17:59:37 INFO mapred.Task: Task:attempt_local860394097_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 17:59:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 17:59:37 INFO mapred.Task: Task attempt_local860394097_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 17:59:37 INFO output.FileOutputCommitter: Saved output of task 'attempt_local860394097_0001_r_000000_0' to hdfs://localhost:9000/user/gtumuluri/oneWordOutput/_temporary/0/task_local860394097_0001_r_000000\n",
      "15/09/15 17:59:37 INFO mapred.LocalJobRunner: Records R/W=8/1 > reduce\n",
      "15/09/15 17:59:37 INFO mapred.Task: Task 'attempt_local860394097_0001_r_000000_0' done.\n",
      "15/09/15 17:59:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local860394097_0001_r_000000_0\n",
      "15/09/15 17:59:37 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 17:59:37 INFO mapreduce.Job: Job job_local860394097_0001 running in uber mode : false\n",
      "15/09/15 17:59:37 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 17:59:38 INFO mapreduce.Job: Job job_local860394097_0001 completed successfully\n",
      "15/09/15 17:59:38 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=212364\n",
      "\t\tFILE: Number of bytes written=796797\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=13\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=107\n",
      "\t\tMap output materialized bytes=129\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=129\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=7\n",
      "\t\tTotal committed heap usage (bytes)=491782144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=13\n",
      "15/09/15 17:59:38 INFO streaming.StreamJob: Output directory: oneWordOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar -mapper '2.2_mapper.py assistance' -reducer 2.2_reducer.py -input enronemail_1h.txt -output oneWordOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2 - Show Output of Single Word Count\n",
    "\n",
    "Passing 'assistance' as a user supplied word to the mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 17:59:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "assistance\t8\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat oneWordOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3 - Mapper for Single Word Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2.3_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.3_mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import string\n",
    "\n",
    "transtable = string.maketrans(\"\",\"\")\n",
    "\n",
    "# Read input from the standard input\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    items = line.split('\\t')\n",
    "    \n",
    "    # If there is no content (as in subject/body in the data), skip\n",
    "    if len(items) < 3:\n",
    "        continue\n",
    "    if items[1] != '0' and items[1] != '1':\n",
    "        continue\n",
    "        \n",
    "    # Output a special word/keyword to allow reducer\n",
    "    # to count the number of times a given class occurs.\n",
    "    # Class is the second field in the data, so output\n",
    "    # that by appending it to the 'class_' keyword string\n",
    "    # and a count of 1 for each occurrence.\n",
    "    print '%s\\t%s' % ('class_' + items[1], 1)    \n",
    "    \n",
    "    # If the line read has just subject, use that, otherwise\n",
    "    # catenate with body also and use the entire content.\n",
    "    if len(items) == 3:\n",
    "        content = items[2]\n",
    "    if len(items) == 4:\n",
    "        content = items[2] + ' ' + items[3]\n",
    "        \n",
    "    # For each word in content, see if the word is same as user\n",
    "    # chosen word, and then output the word and class to which\n",
    "    # the document the word occurred in belongs to. This way, the\n",
    "    # reducer can compute class frequencies for a given word.\n",
    "    content = content.split()\n",
    "    for word in content:\n",
    "        # Remove punctuation\n",
    "        word = word.translate(transtable, string.punctuation)\n",
    "        if word.find(sys.argv[1]) == 0:\n",
    "            print '%s\\t%s' % (word, items[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3 - Reducer for Single Word Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 2.3_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.3_reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "import string\n",
    "\n",
    "transtable = string.maketrans(\"\",\"\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "# Placeholders for the vocabulary, frequencies\n",
    "# Dictionary is of form {vocab_word: {0: x, 1:y}} where\n",
    "# 0 and 1 are classes, and x and y are number of occurrences\n",
    "# of vocab word in respective classes.\n",
    "vocab = {}\n",
    "class0_freq = 0\n",
    "class1_freq = 0\n",
    "\n",
    "# Read each line from standard in and keep adding\n",
    "# class 0 and class 1 occurrences of the word into\n",
    "# the dictionary.\n",
    "for line in sys.stdin:\n",
    "    words = line.strip('')\n",
    "    words = line.split()\n",
    "    if len(words) != 2:\n",
    "        continue\n",
    "    vocab.setdefault(words[0], {0: 0, 1:0})\n",
    "    if int(words[1]):\n",
    "        vocab[words[0]][1] += 1\n",
    "    else:\n",
    "        vocab[words[0]][0] += 1\n",
    "\n",
    "# Class frequencies come in special keywords from the mapper.\n",
    "# Extract them and remove them from the dictionary.\n",
    "class_0_freq = vocab['class_0'][1]\n",
    "class_1_freq = vocab['class_1'][1]\n",
    "vocab.pop('class_0')\n",
    "vocab.pop('class_1')\n",
    "\n",
    "# Compute class probabilities\n",
    "class_0_prob = class_0_freq * 1.0 / (class_0_freq + class_1_freq)\n",
    "class_1_prob = class_1_freq * 1.0 / (class_0_freq + class_1_freq)\n",
    "\n",
    "# Comput size of the vocabulary for each class from the compiled\n",
    "# dictionary above.\n",
    "class_0_vocab = 0\n",
    "class_1_vocab = 0\n",
    "for key in vocab:\n",
    "    class_0_vocab += vocab[key][0]\n",
    "    class_1_vocab += vocab[key][1]\n",
    "\n",
    "# The probability math implemented below to predict class given a document.\n",
    "# P(Spam | Document) > P(Not Spam | Document)\n",
    "# => ln(P(Spam | Document) / P(Not Spam | Document)) > 0\n",
    "# \n",
    "# So, we caclulate this value and the apply the above rule.\n",
    "# ln(P(Spam | Document) / P(Not Spam | Document)) =\n",
    "#   ln(P(Spam) / P(Not Spam)) + SUM(wi) {ln(P(word | Spam)/P(word | Not Spam))}\n",
    "\n",
    "# P(Spam)/P(Not Spam) is always constant. Caclulate and store away.\n",
    "ln_spam_not_spam = math.log(class_1_prob / class_0_prob)\n",
    "\n",
    "# Read each document and compute the prediction using the algorithm above.\n",
    "with open('enronemail_1h.txt') as infile:\n",
    "    for document in infile:\n",
    "        document = document.strip()\n",
    "        document = document.split('\\t')\n",
    "        \n",
    "        # If the document does not have subject/body fields, move on.\n",
    "        if len(document) < 3 or len(document) > 4:\n",
    "            continue\n",
    "            \n",
    "        # If it has the subject and body, catenate the two, otherwise use\n",
    "        # the one available as the whole document.\n",
    "        if len(document) == 4:\n",
    "            content = document[2] + ' ' + document[3]\n",
    "        else:\n",
    "            content = document[2]\n",
    "        \n",
    "        # For each word in the document, compute the probability that the\n",
    "        # word belongs to Spam/Not Spam classes.\n",
    "        content = content.split()\n",
    "        ln_word_spam_word_not_spam = 0\n",
    "        for word in content:\n",
    "            word = word.translate(transtable, string.punctuation)\n",
    "            \n",
    "            # If the word is in vocabulary, grab its frequency (plus one smoothing),\n",
    "            # otherwise, just do plus one smoothing.\n",
    "            if word in vocab:\n",
    "                word_class_1_freq = vocab[word][1] + 1\n",
    "                word_class_0_freq = vocab[word][0] + 1\n",
    "            else:\n",
    "                word_class_1_freq = 0 + 1\n",
    "                word_class_0_freq = 0 + 1\n",
    "            # Summation of the log ratios of word probabilities for each class.\n",
    "            ln_word_spam_word_not_spam += math.log((word_class_1_freq * 1.0 /\n",
    "                                                    (class_1_vocab + len(vocab))) /\n",
    "                                                   (word_class_0_freq * 1.0 /\n",
    "                                                    (class_0_vocab + len(vocab))))\n",
    "        \n",
    "        # The final caculation of the log odds ratio of class. If this ratio is\n",
    "        # greater than zero, we have class 1, otherwise, class 0.\n",
    "        ln_doc_spam_not_spam = ln_spam_not_spam + ln_word_spam_word_not_spam\n",
    "        if ln_doc_spam_not_spam > 0:\n",
    "            print '%s\\t%s\\t%s' % (document[0], document[1], 1)\n",
    "        else:\n",
    "            print '%s\\t%s\\t%s' % (document[0], document[1], 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3 - Run Hadoop MapReduce Single Word Classifier with User Input Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:03:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 18:03:34 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 18:03:34 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 18:03:34 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 18:03:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 18:03:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 18:03:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local205867234_0001\n",
      "15/09/15 18:03:35 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 18:03:35 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 18:03:35 INFO mapreduce.Job: Running job: job_local205867234_0001\n",
      "15/09/15 18:03:35 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 18:03:35 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 18:03:36 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 18:03:36 INFO mapred.LocalJobRunner: Starting task: attempt_local205867234_0001_m_000000_0\n",
      "15/09/15 18:03:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 18:03:36 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/15 18:03:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/15 18:03:36 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/gtumuluri/enronemail_1h.txt:0+203981\n",
      "15/09/15 18:03:36 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 18:03:36 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 18:03:36 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 18:03:36 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 18:03:36 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 18:03:36 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 18:03:36 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/gtumuluri/Documents/BerkeleyMIDS/ScalableML/HW2/./2.3_mapper.py, assistance]\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: Records R/W=101/1\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:03:36 INFO mapred.LocalJobRunner: \n",
      "15/09/15 18:03:36 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 18:03:36 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 18:03:36 INFO mapred.MapTask: bufstart = 0; bufend = 1117; bufvoid = 104857600\n",
      "15/09/15 18:03:36 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213964(104855856); length = 433/6553600\n",
      "15/09/15 18:03:36 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 18:03:36 INFO mapred.Task: Task:attempt_local205867234_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:03:36 INFO mapred.LocalJobRunner: Records R/W=101/1\n",
      "15/09/15 18:03:36 INFO mapred.Task: Task 'attempt_local205867234_0001_m_000000_0' done.\n",
      "15/09/15 18:03:36 INFO mapred.LocalJobRunner: Finishing task: attempt_local205867234_0001_m_000000_0\n",
      "15/09/15 18:03:36 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 18:03:36 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 18:03:36 INFO mapred.LocalJobRunner: Starting task: attempt_local205867234_0001_r_000000_0\n",
      "15/09/15 18:03:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 18:03:36 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/15 18:03:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/15 18:03:36 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3bd68aa3\n",
      "15/09/15 18:03:36 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 18:03:36 INFO reduce.EventFetcher: attempt_local205867234_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 18:03:36 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local205867234_0001_m_000000_0 decomp: 1337 len: 1341 to MEMORY\n",
      "15/09/15 18:03:36 INFO reduce.InMemoryMapOutput: Read 1337 bytes from map-output for attempt_local205867234_0001_m_000000_0\n",
      "15/09/15 18:03:36 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1337, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1337\n",
      "15/09/15 18:03:36 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 18:03:36 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:03:36 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 18:03:36 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:03:36 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1324 bytes\n",
      "15/09/15 18:03:36 INFO reduce.MergeManagerImpl: Merged 1 segments, 1337 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 18:03:36 INFO reduce.MergeManagerImpl: Merging 1 files, 1341 bytes from disk\n",
      "15/09/15 18:03:36 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 18:03:36 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:03:36 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1324 bytes\n",
      "15/09/15 18:03:36 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/gtumuluri/Documents/BerkeleyMIDS/ScalableML/HW2/./2.3_reducer.py]\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 18:03:36 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: Records R/W=109/1\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:03:36 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:03:36 INFO mapreduce.Job: Job job_local205867234_0001 running in uber mode : false\n",
      "15/09/15 18:03:36 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 18:03:37 INFO mapred.Task: Task:attempt_local205867234_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:03:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:03:37 INFO mapred.Task: Task attempt_local205867234_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 18:03:37 INFO output.FileOutputCommitter: Saved output of task 'attempt_local205867234_0001_r_000000_0' to hdfs://localhost:9000/user/gtumuluri/classWordFreqOutput/_temporary/0/task_local205867234_0001_r_000000\n",
      "15/09/15 18:03:37 INFO mapred.LocalJobRunner: Records R/W=109/1 > reduce\n",
      "15/09/15 18:03:37 INFO mapred.Task: Task 'attempt_local205867234_0001_r_000000_0' done.\n",
      "15/09/15 18:03:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local205867234_0001_r_000000_0\n",
      "15/09/15 18:03:37 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 18:03:37 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 18:03:37 INFO mapreduce.Job: Job job_local205867234_0001 completed successfully\n",
      "15/09/15 18:03:37 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=214788\n",
      "\t\tFILE: Number of bytes written=800457\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=2672\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=109\n",
      "\t\tMap output bytes=1117\n",
      "\t\tMap output materialized bytes=1341\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=1341\n",
      "\t\tReduce input records=109\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=218\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=6\n",
      "\t\tTotal committed heap usage (bytes)=488636416\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2672\n",
      "15/09/15 18:03:37 INFO streaming.StreamJob: Output directory: classWordFreqOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar -mapper '2.3_mapper.py assistance' -reducer 2.3_reducer.py -input enronemail_1h.txt -output classWordFreqOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3 - Show Output of Single Word Classifier\n",
    "\n",
    "Passing 'assistance' as the only vocabulary word to classify by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:03:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "0002.2004-08-01.BG\t1\t0\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "0003.2004-08-01.BG\t1\t0\n",
      "0004.1999-12-10.kaminski\t0\t0\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "0004.2004-08-01.BG\t1\t0\n",
      "0005.1999-12-12.kaminski\t0\t0\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "0005.2003-12-18.GP\t1\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "0006.2004-08-01.BG\t1\t0\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "0008.2004-08-01.BG\t1\t0\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\n",
      "0009.2003-12-18.GP\t1\t0\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t0\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0011.2001-06-28.SA_and_HP\t1\t0\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\n",
      "0011.2003-12-18.GP\t1\t0\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "0013.2004-08-01.BG\t1\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\n",
      "0014.2003-12-19.GP\t1\t0\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\n",
      "0015.2003-12-19.GP\t1\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\n",
      "0016.2003-12-19.GP\t1\t0\n",
      "0016.2004-08-01.BG\t1\t0\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "0017.2004-08-01.BG\t1\t0\n",
      "0017.2004-08-02.BG\t1\t0\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0018.2001-07-13.SA_and_HP\t1\t0\n",
      "0018.2003-12-18.GP\t1\t0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat classWordFreqOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.4 - Mapper for Multiple Word Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2.4_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.4_mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import string\n",
    "\n",
    "transtable = string.maketrans(\"\",\"\")\n",
    "\n",
    "# Read input from the standard input\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    items = line.split('\\t')\n",
    "    \n",
    "    # If there is no content (as in subject/body in the data), skip\n",
    "    if len(items) < 3:\n",
    "        continue\n",
    "    if items[1] != '0' and items[1] != '1':\n",
    "        continue\n",
    "    \n",
    "    # Output a special word/keyword to allow reducer\n",
    "    # to count the number of times a given class occurs.\n",
    "    # Class is the second field in the data, so output\n",
    "    # that by appending it to the 'class_' keyword string\n",
    "    # and a count of 1 for each occurrence.\n",
    "    print '%s\\t%s' % ('class_' + items[1], 1)    \n",
    "    if len(items) == 3:\n",
    "        content = items[2]\n",
    "    if len(items) == 4:\n",
    "        content = items[2] + ' ' + items[3]\n",
    "    content = content.split()\n",
    "    \n",
    "    # For each word in content, see if the word is same as user\n",
    "    # chosen word, and then output the word and class to which\n",
    "    # the document the word occurred in belongs to. This way, the\n",
    "    # reducer can compute class frequencies for a given word.\n",
    "    for word in content:\n",
    "        # Remove punctuation\n",
    "        word = word.translate(transtable, string.punctuation)\n",
    "        if word.find(sys.argv[1]) == 0 or word.find(sys.argv[2]) == 0 or word.find(sys.argv[3]) == 0:\n",
    "            print '%s\\t%s' % (word, items[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.4 - Reducer for Multiple Word Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2.4_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.4_reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "import string\n",
    "\n",
    "transtable = string.maketrans(\"\",\"\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "# Placeholders for the vocabulary, frequencies\n",
    "# Dictionary is of form {vocab_word: {0: x, 1:y}} where\n",
    "# 0 and 1 are classes, and x and y are number of occurrences\n",
    "# of vocab word in respective classes.\n",
    "vocab = {}\n",
    "class0_freq = 0\n",
    "class1_freq = 0\n",
    "\n",
    "# Read each line from standard in and keep adding\n",
    "# class 0 and class 1 occurrences of the word into\n",
    "# the dictionary.\n",
    "for line in sys.stdin:\n",
    "    words = line.strip('')\n",
    "    words = line.split()\n",
    "    if len(words) != 2:\n",
    "        continue\n",
    "    vocab.setdefault(words[0], {0: 0, 1:0})\n",
    "    if int(words[1]):\n",
    "        vocab[words[0]][1] += 1\n",
    "    else:\n",
    "        vocab[words[0]][0] += 1\n",
    "\n",
    "# Class frequencies come in special keywords from the mapper.\n",
    "# Extract them and remove them from the dictionary.\n",
    "class_0_freq = vocab['class_0'][1]\n",
    "class_1_freq = vocab['class_1'][1]\n",
    "vocab.pop('class_0')\n",
    "vocab.pop('class_1')\n",
    "\n",
    "# Compute class probabilities\n",
    "class_0_prob = class_0_freq * 1.0 / (class_0_freq + class_1_freq)\n",
    "class_1_prob = class_1_freq * 1.0 / (class_0_freq + class_1_freq)\n",
    "\n",
    "# Comput size of the vocabulary for each class from the compiled\n",
    "# dictionary above.\n",
    "class_0_vocab = 0\n",
    "class_1_vocab = 0\n",
    "for key in vocab:\n",
    "    class_0_vocab += vocab[key][0]\n",
    "    class_1_vocab += vocab[key][1]\n",
    "\n",
    "# The probability math implemented below to predict class given a document.\n",
    "# P(Spam | Document) > P(Not Spam | Document)\n",
    "# => ln(P(Spam | Document) / P(Not Spam | Document)) > 0\n",
    "# \n",
    "# So, we caclulate this value and the apply the above rule.\n",
    "# ln(P(Spam | Document) / P(Not Spam | Document)) =\n",
    "#   ln(P(Spam) / P(Not Spam)) + SUM(wi) {ln(P(word | Spam)/P(word | Not Spam))}\n",
    "\n",
    "# P(Spam)/P(Not Spam) is always constant. Caclulate and store away.\n",
    "ln_spam_not_spam = math.log(class_1_prob / class_0_prob)\n",
    "\n",
    "# Read each document and compute the prediction using the algorithm above.\n",
    "with open('enronemail_1h.txt') as infile:\n",
    "    for document in infile:\n",
    "        document = document.strip()\n",
    "        document = document.split('\\t')\n",
    "        \n",
    "        # If the document does not have subject/body fields, move on.\n",
    "        if len(document) < 3 or len(document) > 4:\n",
    "            continue\n",
    "            \n",
    "        # If it has the subject and body, catenate the two, otherwise use\n",
    "        # the one available as the whole document.\n",
    "        if len(document) == 4:\n",
    "            content = document[2] + ' ' + document[3]\n",
    "        else:\n",
    "            content = document[2]\n",
    "        \n",
    "        # For each word in the document, compute the probability that the\n",
    "        # word belongs to Spam/Not Spam classes.\n",
    "        content = content.split()\n",
    "        ln_word_spam_word_not_spam = 0\n",
    "        for word in content:\n",
    "            word = word.translate(transtable, string.punctuation)\n",
    "            \n",
    "            # If the word is in vocabulary, grab its frequency (plus one smoothing),\n",
    "            # otherwise, just do plus one smoothing.\n",
    "            if word in vocab:\n",
    "                word_class_1_freq = vocab[word][1] + 1\n",
    "                word_class_0_freq = vocab[word][0] + 1\n",
    "            else:\n",
    "                word_class_1_freq = 0 + 1\n",
    "                word_class_0_freq = 0 + 1\n",
    "            # Summation of the log ratios of word probabilities for each class.\n",
    "            ln_word_spam_word_not_spam += math.log((word_class_1_freq * 1.0 /\n",
    "                                                    (class_1_vocab + len(vocab))) /\n",
    "                                                   (word_class_0_freq * 1.0 /\n",
    "                                                    (class_0_vocab + len(vocab))))\n",
    "        \n",
    "        # The final caculation of the log odds ratio of class. If this ratio is\n",
    "        # greater than zero, we have class 1, otherwise, class 0.\n",
    "        ln_doc_spam_not_spam = ln_spam_not_spam + ln_word_spam_word_not_spam\n",
    "        if ln_doc_spam_not_spam > 0:\n",
    "            print '%s\\t%s\\t%s' % (document[0], document[1], 1)\n",
    "        else:\n",
    "            print '%s\\t%s\\t%s' % (document[0], document[1], 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.4 - Run Hadoop MapReduce Multiple Word Classifier with User Input Word\n",
    "\n",
    "Passing three words - assistance, viagra, enlragementWithATypo - as inputs to the mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:04:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 18:04:53 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 18:04:53 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 18:04:53 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 18:04:53 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 18:04:53 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 18:04:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1759235215_0001\n",
      "15/09/15 18:04:54 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 18:04:54 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 18:04:54 INFO mapreduce.Job: Running job: job_local1759235215_0001\n",
      "15/09/15 18:04:54 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 18:04:54 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 18:04:54 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 18:04:54 INFO mapred.LocalJobRunner: Starting task: attempt_local1759235215_0001_m_000000_0\n",
      "15/09/15 18:04:54 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 18:04:54 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/15 18:04:54 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/15 18:04:54 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/gtumuluri/enronemail_1h.txt:0+203981\n",
      "15/09/15 18:04:54 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 18:04:54 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 18:04:54 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 18:04:54 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 18:04:54 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 18:04:54 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 18:04:54 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 18:04:54 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/gtumuluri/Documents/BerkeleyMIDS/ScalableML/HW2/./2.4_mapper.py, assistance, viagra, enlargementWithATypo]\n",
      "15/09/15 18:04:54 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 18:04:54 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 18:04:54 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 18:04:54 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 18:04:54 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 18:04:54 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 18:04:54 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 18:04:54 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 18:04:54 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 18:04:54 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 18:04:54 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 18:04:54 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 18:04:54 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:04:54 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:04:54 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:04:54 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:04:54 INFO streaming.PipeMapRed: Records R/W=101/1\n",
      "15/09/15 18:04:54 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:04:54 INFO mapred.LocalJobRunner: \n",
      "15/09/15 18:04:54 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 18:04:54 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 18:04:54 INFO mapred.MapTask: bufstart = 0; bufend = 1171; bufvoid = 104857600\n",
      "15/09/15 18:04:54 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213940(104855760); length = 457/6553600\n",
      "15/09/15 18:04:54 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 18:04:54 INFO mapred.Task: Task:attempt_local1759235215_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:04:54 INFO mapred.LocalJobRunner: Records R/W=101/1\n",
      "15/09/15 18:04:54 INFO mapred.Task: Task 'attempt_local1759235215_0001_m_000000_0' done.\n",
      "15/09/15 18:04:54 INFO mapred.LocalJobRunner: Finishing task: attempt_local1759235215_0001_m_000000_0\n",
      "15/09/15 18:04:54 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 18:04:54 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 18:04:54 INFO mapred.LocalJobRunner: Starting task: attempt_local1759235215_0001_r_000000_0\n",
      "15/09/15 18:04:55 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 18:04:55 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/15 18:04:55 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/15 18:04:55 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6eccdfd5\n",
      "15/09/15 18:04:55 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 18:04:55 INFO reduce.EventFetcher: attempt_local1759235215_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 18:04:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1759235215_0001_m_000000_0 decomp: 1403 len: 1407 to MEMORY\n",
      "15/09/15 18:04:55 INFO reduce.InMemoryMapOutput: Read 1403 bytes from map-output for attempt_local1759235215_0001_m_000000_0\n",
      "15/09/15 18:04:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1403, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1403\n",
      "15/09/15 18:04:55 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 18:04:55 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:04:55 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 18:04:55 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:04:55 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1390 bytes\n",
      "15/09/15 18:04:55 INFO reduce.MergeManagerImpl: Merged 1 segments, 1403 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 18:04:55 INFO reduce.MergeManagerImpl: Merging 1 files, 1407 bytes from disk\n",
      "15/09/15 18:04:55 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 18:04:55 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:04:55 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1390 bytes\n",
      "15/09/15 18:04:55 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:04:55 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/gtumuluri/Documents/BerkeleyMIDS/ScalableML/HW2/./2.4_reducer.py]\n",
      "15/09/15 18:04:55 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 18:04:55 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 18:04:55 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:04:55 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:04:55 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:04:55 INFO streaming.PipeMapRed: Records R/W=115/1\n",
      "15/09/15 18:04:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:04:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:04:55 INFO mapreduce.Job: Job job_local1759235215_0001 running in uber mode : false\n",
      "15/09/15 18:04:55 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 18:04:55 INFO mapred.Task: Task:attempt_local1759235215_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:04:55 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:04:55 INFO mapred.Task: Task attempt_local1759235215_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 18:04:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1759235215_0001_r_000000_0' to hdfs://localhost:9000/user/gtumuluri/classMultiWordFreqOutput/_temporary/0/task_local1759235215_0001_r_000000\n",
      "15/09/15 18:04:55 INFO mapred.LocalJobRunner: Records R/W=115/1 > reduce\n",
      "15/09/15 18:04:55 INFO mapred.Task: Task 'attempt_local1759235215_0001_r_000000_0' done.\n",
      "15/09/15 18:04:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local1759235215_0001_r_000000_0\n",
      "15/09/15 18:04:55 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 18:04:56 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 18:04:56 INFO mapreduce.Job: Job job_local1759235215_0001 completed successfully\n",
      "15/09/15 18:04:56 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=214920\n",
      "\t\tFILE: Number of bytes written=803775\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=2672\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=115\n",
      "\t\tMap output bytes=1171\n",
      "\t\tMap output materialized bytes=1407\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=1407\n",
      "\t\tReduce input records=115\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=230\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=6\n",
      "\t\tTotal committed heap usage (bytes)=492830720\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2672\n",
      "15/09/15 18:04:56 INFO streaming.StreamJob: Output directory: classMultiWordFreqOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar -mapper '2.4_mapper.py assistance viagra enlargementWithATypo' -reducer 2.4_reducer.py -input enronemail_1h.txt -output classMultiWordFreqOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.4 - Show Output of Multiple Word Classifier\n",
    "\n",
    "Passing 'assistance' as the only vocabulary word to classify by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:05:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "0002.2004-08-01.BG\t1\t0\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "0003.2004-08-01.BG\t1\t0\n",
      "0004.1999-12-10.kaminski\t0\t0\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "0004.2004-08-01.BG\t1\t0\n",
      "0005.1999-12-12.kaminski\t0\t0\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "0005.2003-12-18.GP\t1\t0\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "0006.2004-08-01.BG\t1\t0\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "0008.2004-08-01.BG\t1\t0\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\n",
      "0009.2003-12-18.GP\t1\t0\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t0\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0011.2001-06-28.SA_and_HP\t1\t0\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\n",
      "0011.2003-12-18.GP\t1\t0\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "0013.2004-08-01.BG\t1\t0\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\n",
      "0014.2003-12-19.GP\t1\t0\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\n",
      "0015.2003-12-19.GP\t1\t0\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\n",
      "0016.2003-12-19.GP\t1\t0\n",
      "0016.2004-08-01.BG\t1\t0\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "0017.2004-08-01.BG\t1\t0\n",
      "0017.2004-08-02.BG\t1\t0\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0018.2001-07-13.SA_and_HP\t1\t0\n",
      "0018.2003-12-18.GP\t1\t0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat classMultiWordFreqOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.5 - Mapper for Full Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 2.5_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.5_mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import string\n",
    "\n",
    "transtable = string.maketrans(\"\",\"\")\n",
    "\n",
    "# Read input from the standard input\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    items = line.split('\\t')\n",
    "    \n",
    "    # If there is no content (as in subject/body in the data), skip\n",
    "    if len(items) < 3:\n",
    "        continue\n",
    "    if items[1] != '0' and items[1] != '1':\n",
    "        continue\n",
    "    \n",
    "    # Output a special word/keyword to allow reducer\n",
    "    # to count the number of times a given class occurs.\n",
    "    # Class is the second field in the data, so output\n",
    "    # that by appending it to the 'class_' keyword string\n",
    "    # and a count of 1 for each occurrence.\n",
    "    print '%s\\t%s' % ('class_' + items[1], 1)    \n",
    "    if len(items) == 3:\n",
    "        content = items[2]\n",
    "    if len(items) == 4:\n",
    "        content = items[2] + ' ' + items[3]\n",
    "    content = content.split()\n",
    "    \n",
    "    # For each word in content, see if the word is same as user\n",
    "    # chosen word, and then output the word and class to which\n",
    "    # the document the word occurred in belongs to. This way, the\n",
    "    # reducer can compute class frequencies for a given word.\n",
    "    for word in content:\n",
    "        # Remove punctuation\n",
    "        word = word.translate(transtable, string.punctuation)\n",
    "        print '%s\\t%s' % (word, items[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.5 - Reducer for Full Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2.5_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.5_reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "import string\n",
    "\n",
    "transtable = string.maketrans(\"\",\"\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "# Placeholders for the vocabulary, frequencies\n",
    "# Dictionary is of form {vocab_word: {0: x, 1:y}} where\n",
    "# 0 and 1 are classes, and x and y are number of occurrences\n",
    "# of vocab word in respective classes.\n",
    "vocab = {}\n",
    "class0_freq = 0\n",
    "class1_freq = 0\n",
    "\n",
    "# Read each line from standard in and keep adding\n",
    "# class 0 and class 1 occurrences of the word into\n",
    "# the dictionary.\n",
    "for line in sys.stdin:\n",
    "    words = line.strip('')\n",
    "    words = line.split()\n",
    "    if len(words) != 2:\n",
    "        continue\n",
    "    vocab.setdefault(words[0], {0: 0, 1:0})\n",
    "    if int(words[1]):\n",
    "        vocab[words[0]][1] += 1\n",
    "    else:\n",
    "        vocab[words[0]][0] += 1\n",
    "\n",
    "# Class frequencies come in special keywords from the mapper.\n",
    "# Extract them and remove them from the dictionary.\n",
    "class_0_freq = vocab['class_0'][1]\n",
    "class_1_freq = vocab['class_1'][1]\n",
    "vocab.pop('class_0')\n",
    "vocab.pop('class_1')\n",
    "\n",
    "# Compute class probabilities\n",
    "class_0_prob = class_0_freq * 1.0 / (class_0_freq + class_1_freq)\n",
    "class_1_prob = class_1_freq * 1.0 / (class_0_freq + class_1_freq)\n",
    "\n",
    "# Comput size of the vocabulary for each class from the compiled\n",
    "# dictionary above.\n",
    "class_0_vocab = 0\n",
    "class_1_vocab = 0\n",
    "for key in vocab:\n",
    "    class_0_vocab += vocab[key][0]\n",
    "    class_1_vocab += vocab[key][1]\n",
    "\n",
    "# The probability math implemented below to predict class given a document.\n",
    "# P(Spam | Document) > P(Not Spam | Document)\n",
    "# => ln(P(Spam | Document) / P(Not Spam | Document)) > 0\n",
    "# \n",
    "# So, we caclulate this value and the apply the above rule.\n",
    "# ln(P(Spam | Document) / P(Not Spam | Document)) =\n",
    "#   ln(P(Spam) / P(Not Spam)) + SUM(wi) {ln(P(word | Spam)/P(word | Not Spam))}\n",
    "\n",
    "# P(Spam)/P(Not Spam) is always constant. Caclulate and store away.\n",
    "ln_spam_not_spam = math.log(class_1_prob / class_0_prob)\n",
    "\n",
    "# Read each document and compute the prediction using the algorithm above.\n",
    "with open('enronemail_1h.txt') as infile:\n",
    "    for document in infile:\n",
    "        document = document.strip()\n",
    "        document = document.split('\\t')\n",
    "        \n",
    "        # If the document does not have subject/body fields, move on.\n",
    "        if len(document) < 3 or len(document) > 4:\n",
    "            continue\n",
    "            \n",
    "        # If it has the subject and body, catenate the two, otherwise use\n",
    "        # the one available as the whole document.\n",
    "        if len(document) == 4:\n",
    "            content = document[2] + ' ' + document[3]\n",
    "        else:\n",
    "            content = document[2]\n",
    "        \n",
    "        # For each word in the document, compute the probability that the\n",
    "        # word belongs to Spam/Not Spam classes.\n",
    "        content = content.split()\n",
    "        ln_word_spam_word_not_spam = 0\n",
    "        for word in content:\n",
    "            word = word.translate(transtable, string.punctuation)\n",
    "            \n",
    "            # If the word is in vocabulary, grab its frequency (plus one smoothing),\n",
    "            # otherwise, just do plus one smoothing.\n",
    "            if word in vocab:\n",
    "                word_class_1_freq = vocab[word][1] + 1\n",
    "                word_class_0_freq = vocab[word][0] + 1\n",
    "            else:\n",
    "                word_class_1_freq = 0 + 1\n",
    "                word_class_0_freq = 0 + 1\n",
    "            # Summation of the log ratios of word probabilities for each class.\n",
    "            ln_word_spam_word_not_spam += math.log((word_class_1_freq * 1.0 /\n",
    "                                                    (class_1_vocab + len(vocab))) /\n",
    "                                                   (word_class_0_freq * 1.0 /\n",
    "                                                    (class_0_vocab + len(vocab))))\n",
    "        \n",
    "        # The final caculation of the log odds ratio of class. If this ratio is\n",
    "        # greater than zero, we have class 1, otherwise, class 0.\n",
    "        ln_doc_spam_not_spam = ln_spam_not_spam + ln_word_spam_word_not_spam\n",
    "        if ln_doc_spam_not_spam > 0:\n",
    "            print '%s\\t%s\\t%s' % (document[0], document[1], 1)\n",
    "        else:\n",
    "            print '%s\\t%s\\t%s' % (document[0], document[1], 0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.5 - Run Hadoop MapReduce Full Naive Bayes Classifier\n",
    "\n",
    "No words are passed as arguments in this case as we will use the full vocabulary to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:06:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 18:06:06 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 18:06:06 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 18:06:06 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 18:06:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 18:06:06 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 18:06:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1826367675_0001\n",
      "15/09/15 18:06:07 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 18:06:07 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 18:06:07 INFO mapreduce.Job: Running job: job_local1826367675_0001\n",
      "15/09/15 18:06:07 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 18:06:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 18:06:07 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 18:06:07 INFO mapred.LocalJobRunner: Starting task: attempt_local1826367675_0001_m_000000_0\n",
      "15/09/15 18:06:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 18:06:07 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/15 18:06:07 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/15 18:06:07 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/gtumuluri/enronemail_1h.txt:0+203981\n",
      "15/09/15 18:06:07 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 18:06:07 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 18:06:07 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 18:06:07 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 18:06:07 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 18:06:07 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 18:06:07 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 18:06:07 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/gtumuluri/Documents/BerkeleyMIDS/ScalableML/HW2/./2.5_mapper.py]\n",
      "15/09/15 18:06:07 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 18:06:07 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 18:06:07 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 18:06:07 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 18:06:07 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 18:06:07 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 18:06:07 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 18:06:07 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 18:06:07 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 18:06:07 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 18:06:07 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 18:06:07 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 18:06:07 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:06:07 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:06:07 INFO streaming.PipeMapRed: Records R/W=73/1\n",
      "15/09/15 18:06:07 INFO streaming.PipeMapRed: R/W/S=100/14975/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:06:07 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:06:07 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:06:07 INFO mapred.LocalJobRunner: \n",
      "15/09/15 18:06:07 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 18:06:07 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 18:06:07 INFO mapred.MapTask: bufstart = 0; bufend = 249750; bufvoid = 104857600\n",
      "15/09/15 18:06:07 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26083984(104335936); length = 130413/6553600\n",
      "15/09/15 18:06:08 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 18:06:08 INFO mapred.Task: Task:attempt_local1826367675_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:06:08 INFO mapred.LocalJobRunner: Records R/W=73/1\n",
      "15/09/15 18:06:08 INFO mapred.Task: Task 'attempt_local1826367675_0001_m_000000_0' done.\n",
      "15/09/15 18:06:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local1826367675_0001_m_000000_0\n",
      "15/09/15 18:06:08 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 18:06:08 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 18:06:08 INFO mapred.LocalJobRunner: Starting task: attempt_local1826367675_0001_r_000000_0\n",
      "15/09/15 18:06:08 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 18:06:08 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/15 18:06:08 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/15 18:06:08 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3bd68aa3\n",
      "15/09/15 18:06:08 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 18:06:08 INFO reduce.EventFetcher: attempt_local1826367675_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 18:06:08 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1826367675_0001_m_000000_0 decomp: 314960 len: 314964 to MEMORY\n",
      "15/09/15 18:06:08 INFO reduce.InMemoryMapOutput: Read 314960 bytes from map-output for attempt_local1826367675_0001_m_000000_0\n",
      "15/09/15 18:06:08 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 314960, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->314960\n",
      "15/09/15 18:06:08 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 18:06:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:06:08 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 18:06:08 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:06:08 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 314957 bytes\n",
      "15/09/15 18:06:08 INFO mapreduce.Job: Job job_local1826367675_0001 running in uber mode : false\n",
      "15/09/15 18:06:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 18:06:08 INFO reduce.MergeManagerImpl: Merged 1 segments, 314960 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 18:06:08 INFO reduce.MergeManagerImpl: Merging 1 files, 314964 bytes from disk\n",
      "15/09/15 18:06:08 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 18:06:08 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:06:08 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 314957 bytes\n",
      "15/09/15 18:06:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:06:08 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/gtumuluri/Documents/BerkeleyMIDS/ScalableML/HW2/./2.5_reducer.py]\n",
      "15/09/15 18:06:08 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 18:06:08 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 18:06:08 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:06:08 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:06:08 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:06:08 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:06:08 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:06:08 INFO streaming.PipeMapRed: Records R/W=32604/1\n",
      "15/09/15 18:06:08 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:06:08 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:06:08 INFO mapred.Task: Task:attempt_local1826367675_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:06:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:06:08 INFO mapred.Task: Task attempt_local1826367675_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 18:06:08 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1826367675_0001_r_000000_0' to hdfs://localhost:9000/user/gtumuluri/fullClassificationOutput/_temporary/0/task_local1826367675_0001_r_000000\n",
      "15/09/15 18:06:08 INFO mapred.LocalJobRunner: Records R/W=32604/1 > reduce\n",
      "15/09/15 18:06:08 INFO mapred.Task: Task 'attempt_local1826367675_0001_r_000000_0' done.\n",
      "15/09/15 18:06:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local1826367675_0001_r_000000_0\n",
      "15/09/15 18:06:08 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 18:06:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 18:06:09 INFO mapreduce.Job: Job job_local1826367675_0001 completed successfully\n",
      "15/09/15 18:06:09 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=842034\n",
      "\t\tFILE: Number of bytes written=1744290\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=2672\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=32604\n",
      "\t\tMap output bytes=249750\n",
      "\t\tMap output materialized bytes=314964\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5744\n",
      "\t\tReduce shuffle bytes=314964\n",
      "\t\tReduce input records=32604\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=65208\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=488636416\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2672\n",
      "15/09/15 18:06:09 INFO streaming.StreamJob: Output directory: fullClassificationOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar -mapper 2.5_mapper.py -reducer 2.5_reducer.py -input enronemail_1h.txt -output fullClassificationOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.5 - Show Output of Full Naive Bayes Classifier\n",
    "\n",
    "No words are passed as argument. Training and prediction uses full vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:06:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\n",
      "0002.2003-12-18.GP\t1\t1\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0003.2003-12-18.GP\t1\t1\n",
      "0003.2004-08-01.BG\t1\t1\n",
      "0004.1999-12-10.kaminski\t0\t0\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\n",
      "0004.2004-08-01.BG\t1\t1\n",
      "0005.1999-12-12.kaminski\t0\t0\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\n",
      "0005.2003-12-18.GP\t1\t1\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\n",
      "0006.2003-12-18.GP\t1\t1\n",
      "0006.2004-08-01.BG\t1\t1\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0007.2003-12-18.GP\t1\t1\n",
      "0007.2004-08-01.BG\t1\t1\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\n",
      "0008.2003-12-18.GP\t1\t1\n",
      "0008.2004-08-01.BG\t1\t1\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\n",
      "0009.2003-12-18.GP\t1\t1\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0010.2003-12-18.GP\t1\t1\n",
      "0010.2004-08-01.BG\t1\t1\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\n",
      "0011.2003-12-18.GP\t1\t1\n",
      "0011.2004-08-01.BG\t1\t1\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0012.2003-12-19.GP\t1\t1\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\n",
      "0014.2003-12-19.GP\t1\t1\n",
      "0014.2004-08-01.BG\t1\t1\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\n",
      "0015.2003-12-19.GP\t1\t1\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\n",
      "0016.2003-12-19.GP\t1\t1\n",
      "0016.2004-08-01.BG\t1\t1\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0017.2003-12-18.GP\t1\t1\n",
      "0017.2004-08-01.BG\t1\t1\n",
      "0017.2004-08-02.BG\t1\t1\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0018.2003-12-18.GP\t1\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat fullClassificationOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.5b - Reducer for Full Naive Bayes Classification REMOVE Infrequent Words\n",
    "\n",
    "In this section, we re-do the classification of full Naive Bayes by dropping words that occur less than three times in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2.5b_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.5b_reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import math\n",
    "import string\n",
    "\n",
    "transtable = string.maketrans(\"\",\"\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "# Placeholders for the vocabulary, frequencies\n",
    "# Dictionary is of form {vocab_word: {0: x, 1:y}} where\n",
    "# 0 and 1 are classes, and x and y are number of occurrences\n",
    "# of vocab word in respective classes.\n",
    "vocab = {}\n",
    "class0_freq = 0\n",
    "class1_freq = 0\n",
    "\n",
    "# Read each line from standard in and keep adding\n",
    "# class 0 and class 1 occurrences of the word into\n",
    "# the dictionary.\n",
    "for line in sys.stdin:\n",
    "    words = line.strip('')\n",
    "    words = line.split()\n",
    "    if len(words) != 2:\n",
    "        continue\n",
    "    vocab.setdefault(words[0], {0: 0, 1:0})\n",
    "    if int(words[1]):\n",
    "        vocab[words[0]][1] += 1\n",
    "    else:\n",
    "        vocab[words[0]][0] += 1\n",
    "\n",
    "# FIGURE OUT WHICH WORDS OCCUR WITH A FREQ OF LESS THAN 3\n",
    "# AND REMOVE THEM FROM THE VOCABULARY.\n",
    "exclude_list = []\n",
    "for key in vocab:\n",
    "    if sum(vocab[key].values()) < 3:\n",
    "        exclude_list.append(key)\n",
    "for word in exclude_list:\n",
    "    vocab.pop(word)\n",
    "\n",
    "# Class frequencies come in special keywords from the mapper.\n",
    "# Extract them and remove them from the dictionary.\n",
    "class_0_freq = vocab['class_0'][1]\n",
    "class_1_freq = vocab['class_1'][1]\n",
    "vocab.pop('class_0')\n",
    "vocab.pop('class_1')\n",
    "\n",
    "# Compute class probabilities\n",
    "class_0_prob = class_0_freq * 1.0 / (class_0_freq + class_1_freq)\n",
    "class_1_prob = class_1_freq * 1.0 / (class_0_freq + class_1_freq)\n",
    "\n",
    "# Comput size of the vocabulary for each class from the compiled\n",
    "# dictionary above.\n",
    "class_0_vocab = 0\n",
    "class_1_vocab = 0\n",
    "for key in vocab:\n",
    "    class_0_vocab += vocab[key][0]\n",
    "    class_1_vocab += vocab[key][1]\n",
    "\n",
    "# The probability math implemented below to predict class given a document.\n",
    "# P(Spam | Document) > P(Not Spam | Document)\n",
    "# => ln(P(Spam | Document) / P(Not Spam | Document)) > 0\n",
    "# \n",
    "# So, we caclulate this value and the apply the above rule.\n",
    "# ln(P(Spam | Document) / P(Not Spam | Document)) =\n",
    "#   ln(P(Spam) / P(Not Spam)) + SUM(wi) {ln(P(word | Spam)/P(word | Not Spam))}\n",
    "\n",
    "# P(Spam)/P(Not Spam) is always constant. Caclulate and store away.\n",
    "ln_spam_not_spam = math.log(class_1_prob / class_0_prob)\n",
    "\n",
    "# Read each document and compute the prediction using the algorithm above.\n",
    "with open('enronemail_1h.txt') as infile:\n",
    "    for document in infile:\n",
    "        document = document.strip()\n",
    "        document = document.split('\\t')\n",
    "        \n",
    "        # If the document does not have subject/body fields, move on.\n",
    "        if len(document) < 3 or len(document) > 4:\n",
    "            continue\n",
    "            \n",
    "        # If it has the subject and body, catenate the two, otherwise use\n",
    "        # the one available as the whole document.\n",
    "        if len(document) == 4:\n",
    "            content = document[2] + ' ' + document[3]\n",
    "        else:\n",
    "            content = document[2]\n",
    "        \n",
    "        # For each word in the document, compute the probability that the\n",
    "        # word belongs to Spam/Not Spam classes.\n",
    "        content = content.split()\n",
    "        ln_word_spam_word_not_spam = 0\n",
    "        for word in content:\n",
    "            word = word.translate(transtable, string.punctuation)\n",
    "            \n",
    "            # If the word is in vocabulary, grab its frequency (plus one smoothing),\n",
    "            # otherwise, just do plus one smoothing.\n",
    "            if word in vocab:\n",
    "                word_class_1_freq = vocab[word][1] + 1\n",
    "                word_class_0_freq = vocab[word][0] + 1\n",
    "            else:\n",
    "                word_class_1_freq = 0 + 1\n",
    "                word_class_0_freq = 0 + 1\n",
    "            # Summation of the log ratios of word probabilities for each class.\n",
    "            ln_word_spam_word_not_spam += math.log((word_class_1_freq * 1.0 /\n",
    "                                                    (class_1_vocab + len(vocab))) /\n",
    "                                                   (word_class_0_freq * 1.0 /\n",
    "                                                    (class_0_vocab + len(vocab))))\n",
    "        \n",
    "        # The final caculation of the log odds ratio of class. If this ratio is\n",
    "        # greater than zero, we have class 1, otherwise, class 0.\n",
    "        ln_doc_spam_not_spam = ln_spam_not_spam + ln_word_spam_word_not_spam\n",
    "        if ln_doc_spam_not_spam > 0:\n",
    "            print '%s\\t%s\\t%s' % (document[0], document[1], 1)\n",
    "        else:\n",
    "            print '%s\\t%s\\t%s' % (document[0], document[1], 0)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.5b - Run Hadoop MapReduce Full Naive Bayes Classifier REMOVE Infrequent Words\n",
    "\n",
    "No words are passed as arguments in this case as we will use the full vocabulary to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:07:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/15 18:07:06 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/09/15 18:07:06 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/09/15 18:07:06 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/09/15 18:07:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/15 18:07:06 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/09/15 18:07:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1207432264_0001\n",
      "15/09/15 18:07:07 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/09/15 18:07:07 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/09/15 18:07:07 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/09/15 18:07:07 INFO mapreduce.Job: Running job: job_local1207432264_0001\n",
      "15/09/15 18:07:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 18:07:07 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/09/15 18:07:07 INFO mapred.LocalJobRunner: Starting task: attempt_local1207432264_0001_m_000000_0\n",
      "15/09/15 18:07:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 18:07:07 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/15 18:07:07 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/15 18:07:07 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/gtumuluri/enronemail_1h.txt:0+203981\n",
      "15/09/15 18:07:07 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/09/15 18:07:07 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/09/15 18:07:07 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/09/15 18:07:07 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/09/15 18:07:07 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/09/15 18:07:07 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/09/15 18:07:07 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/09/15 18:07:07 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/gtumuluri/Documents/BerkeleyMIDS/ScalableML/HW2/./2.5_mapper.py]\n",
      "15/09/15 18:07:07 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/09/15 18:07:07 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/09/15 18:07:07 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/09/15 18:07:07 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/09/15 18:07:07 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/09/15 18:07:07 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/09/15 18:07:07 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/09/15 18:07:07 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/09/15 18:07:07 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/09/15 18:07:07 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/09/15 18:07:07 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/09/15 18:07:07 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/09/15 18:07:07 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:07:07 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:07:07 INFO streaming.PipeMapRed: Records R/W=73/1\n",
      "15/09/15 18:07:07 INFO streaming.PipeMapRed: R/W/S=100/10619/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:07:07 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:07:07 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:07:07 INFO mapred.LocalJobRunner: \n",
      "15/09/15 18:07:07 INFO mapred.MapTask: Starting flush of map output\n",
      "15/09/15 18:07:07 INFO mapred.MapTask: Spilling map output\n",
      "15/09/15 18:07:07 INFO mapred.MapTask: bufstart = 0; bufend = 249750; bufvoid = 104857600\n",
      "15/09/15 18:07:07 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26083984(104335936); length = 130413/6553600\n",
      "15/09/15 18:07:07 INFO mapred.MapTask: Finished spill 0\n",
      "15/09/15 18:07:07 INFO mapred.Task: Task:attempt_local1207432264_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:07:07 INFO mapred.LocalJobRunner: Records R/W=73/1\n",
      "15/09/15 18:07:07 INFO mapred.Task: Task 'attempt_local1207432264_0001_m_000000_0' done.\n",
      "15/09/15 18:07:07 INFO mapred.LocalJobRunner: Finishing task: attempt_local1207432264_0001_m_000000_0\n",
      "15/09/15 18:07:07 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/09/15 18:07:07 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/09/15 18:07:07 INFO mapred.LocalJobRunner: Starting task: attempt_local1207432264_0001_r_000000_0\n",
      "15/09/15 18:07:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/09/15 18:07:07 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/09/15 18:07:07 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/09/15 18:07:07 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6eccdfd5\n",
      "15/09/15 18:07:07 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/09/15 18:07:07 INFO reduce.EventFetcher: attempt_local1207432264_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/09/15 18:07:07 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1207432264_0001_m_000000_0 decomp: 314960 len: 314964 to MEMORY\n",
      "15/09/15 18:07:08 INFO reduce.InMemoryMapOutput: Read 314960 bytes from map-output for attempt_local1207432264_0001_m_000000_0\n",
      "15/09/15 18:07:08 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 314960, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->314960\n",
      "15/09/15 18:07:08 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/09/15 18:07:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:07:08 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/09/15 18:07:08 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:07:08 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 314957 bytes\n",
      "15/09/15 18:07:08 INFO mapreduce.Job: Job job_local1207432264_0001 running in uber mode : false\n",
      "15/09/15 18:07:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/09/15 18:07:08 INFO reduce.MergeManagerImpl: Merged 1 segments, 314960 bytes to disk to satisfy reduce memory limit\n",
      "15/09/15 18:07:08 INFO reduce.MergeManagerImpl: Merging 1 files, 314964 bytes from disk\n",
      "15/09/15 18:07:08 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/09/15 18:07:08 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/09/15 18:07:08 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 314957 bytes\n",
      "15/09/15 18:07:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:07:08 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/gtumuluri/Documents/BerkeleyMIDS/ScalableML/HW2/./2.5b_reducer.py]\n",
      "15/09/15 18:07:08 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/09/15 18:07:08 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/09/15 18:07:08 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:07:08 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:07:08 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:07:08 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:07:08 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/09/15 18:07:08 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/09/15 18:07:08 INFO streaming.PipeMapRed: Records R/W=32604/1\n",
      "15/09/15 18:07:08 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/09/15 18:07:08 INFO mapred.Task: Task:attempt_local1207432264_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/09/15 18:07:08 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "15/09/15 18:07:08 INFO mapred.Task: Task attempt_local1207432264_0001_r_000000_0 is allowed to commit now\n",
      "15/09/15 18:07:08 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1207432264_0001_r_000000_0' to hdfs://localhost:9000/user/gtumuluri/fullClassificationOutputExcludeInfreq/_temporary/0/task_local1207432264_0001_r_000000\n",
      "15/09/15 18:07:08 INFO mapred.LocalJobRunner: Records R/W=32604/1 > reduce\n",
      "15/09/15 18:07:08 INFO mapred.Task: Task 'attempt_local1207432264_0001_r_000000_0' done.\n",
      "15/09/15 18:07:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local1207432264_0001_r_000000_0\n",
      "15/09/15 18:07:08 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/09/15 18:07:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/09/15 18:07:09 INFO mapreduce.Job: Job job_local1207432264_0001 completed successfully\n",
      "15/09/15 18:07:09 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=842034\n",
      "\t\tFILE: Number of bytes written=1744346\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=2672\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=32604\n",
      "\t\tMap output bytes=249750\n",
      "\t\tMap output materialized bytes=314964\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5744\n",
      "\t\tReduce shuffle bytes=314964\n",
      "\t\tReduce input records=32604\n",
      "\t\tReduce output records=100\n",
      "\t\tSpilled Records=65208\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=491782144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2672\n",
      "15/09/15 18:07:09 INFO streaming.StreamJob: Output directory: fullClassificationOutputExcludeInfreq\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar -mapper 2.5_mapper.py -reducer 2.5b_reducer.py -input enronemail_1h.txt -output fullClassificationOutputExcludeInfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.5b - Show Output of Full Naive Bayes Classifier REMOVE Infrequent Words\n",
    "\n",
    "No words are passed as argument. Training and prediction uses full vocabulary AFTER excluding any words occurring 3 times or less in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/09/15 18:07:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\n",
      "0002.2003-12-18.GP\t1\t1\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "0003.2003-12-18.GP\t1\t1\n",
      "0003.2004-08-01.BG\t1\t1\n",
      "0004.1999-12-10.kaminski\t0\t0\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\n",
      "0004.2004-08-01.BG\t1\t1\n",
      "0005.1999-12-12.kaminski\t0\t0\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\n",
      "0005.2003-12-18.GP\t1\t1\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\n",
      "0006.2003-12-18.GP\t1\t1\n",
      "0006.2004-08-01.BG\t1\t1\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "0007.2003-12-18.GP\t1\t1\n",
      "0007.2004-08-01.BG\t1\t1\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\n",
      "0008.2003-12-18.GP\t1\t1\n",
      "0008.2004-08-01.BG\t1\t1\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\n",
      "0009.2003-12-18.GP\t1\t1\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "0010.2004-08-01.BG\t1\t1\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\n",
      "0011.2003-12-18.GP\t1\t1\n",
      "0011.2004-08-01.BG\t1\t1\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "0012.2003-12-19.GP\t1\t1\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\n",
      "0014.2003-12-19.GP\t1\t1\n",
      "0014.2004-08-01.BG\t1\t1\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\n",
      "0015.2003-12-19.GP\t1\t1\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\n",
      "0016.2003-12-19.GP\t1\t1\n",
      "0016.2004-08-01.BG\t1\t1\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "0017.2003-12-18.GP\t1\t1\n",
      "0017.2004-08-01.BG\t1\t0\n",
      "0017.2004-08-02.BG\t1\t1\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0018.2003-12-18.GP\t1\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat fullClassificationOutputExcludeInfreq/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.6 - SKLearn Bernoulli and Multinomial Naive Bayes\n",
    "\n",
    "In this case, we use the built-in SKLearn classifier training models and do prediction on the same data as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Accuracy: 0.77\n",
      "Multinomial Naive Bayes Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "######## PROBLEM NUMBER 1.6 ##########\n",
    "######################################\n",
    "\n",
    "# SKLearn benchmark with NaiveBayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Read the data set as a pandas dataframe and add a new column that\n",
    "# combines the text of subject line and email body. Also, drop any\n",
    "# rows that have NAs in key content and class columns.\n",
    "enron = pd.read_csv('enronemail_1h.txt', sep = '\\t', header = None)\n",
    "enron = enron.dropna(subset = [1, 2, 3])\n",
    "enron.loc[:, 'content'] = enron.loc[:, 2] + ' ' + enron.loc[:, 3]\n",
    "\n",
    "# Extract columns into text content and labels. Remember, we will train\n",
    "# and test on the same exact data - there is separate 'test' data.\n",
    "train_labels = enron.loc[:, 1]\n",
    "train_content = enron.loc[:, 'content']\n",
    "test_labels = enron.loc[:, 1]\n",
    "test_content = enron.loc[:, 'content']\n",
    "\n",
    "# Transform text into features for training\n",
    "count_vect = CountVectorizer()\n",
    "train_features = count_vect.fit_transform(train_content)\n",
    "test_features = count_vect.transform(train_content)\n",
    "\n",
    "# Train a Bernoulli Naive Bayes model with defaults and measure prediction\n",
    "# accuracy on the same data. Print the output.\n",
    "bern = BernoulliNB()\n",
    "bern.fit(train_features, train_labels)\n",
    "predictions = bern.predict(test_features)\n",
    "accuracy = float(len([i for i, j in \n",
    "                      zip(predictions, test_labels)\n",
    "                      if i == j])) / len(test_labels)\n",
    "print \"Bernoulli Naive Bayes Accuracy: \" + str(round(accuracy, 2))\n",
    "\n",
    "# Train a Multinomial Naive Bayes model with defaults and measure prediction\n",
    "# accuracy on the same data. Print the output.\n",
    "mult = MultinomialNB()\n",
    "mult.fit(train_features, train_labels)\n",
    "predictions = mult.predict(test_features)\n",
    "accuracy = float(len([i for i, j in zip(predictions, test_labels) if i == j])) / len(test_labels)\n",
    "print \"Multinomial Naive Bayes Accuracy: \" + str(round(accuracy, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Yarn and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "15/09/15 18:07:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "15/09/15 18:08:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.0/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.0/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
